\appendix
\chapter{Demostración de la cota de VC}
\label{apendice:demVC}
Se ha comentado en~\ref{th:VCBound} que la demostración del la cota de
generalización de VC es muy técnica. Por ello, se expone aquí la demostración
que viene en el libro~\citep{Abu-Mostafa:2012:LD:2207825}.

\begin{teorema}[Vapnik, Chervonenkis, 1971]
\label{th:A1}
   \begin{displaymath}
       \mathbb{P} \left [\sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \leq 4m_{\mathcal{H}}(2N)e^{-\frac{1}{8} \epsilon^2 N}
   \end{displaymath}
\end{teorema}

Este resultado es la desigualdad de VC, e implica directamente la cota de VC
del Teorema~\ref{th:VCBound}. La desigualdad es válida para cualquier función
objetivo, ya sea determinista o probabilística, y para cualquier distribución
de los datos de entrada. La probabilidad se da en función de conjuntos de datos
de tamaño $N$. Cada dato se genera independiente e idénticamente distribuido,
donde cada punto sigue la distribución conjunta $P(x,y)$. El evento
$\sup_{h \in \mathcal{H}} | E_{in}(h) - E{out}(h) | > \epsilon$ es equivalente
a la unión sobre todas las $h \in \mathcal{H}$ del evento
$| E_{in}(h) - E_{out}(h) | > \epsilon$. Esta unión contiene el evento que
implica $g$ en~\ref{th:VCBound}. El uso del supremo es necesario ya que
$\mathcal{H}$ puede tener un conjunto denso de hipótesis.

El principal reto para probar este teorema es que $E_{out}(h)$ es difícil de
manipular con respecto a $E_{in}(h)$, ya que $E_{out}(h)$ depende de todo el
espacio de entrada y no sólo de un conjunto finito de puntos. El primer paso
para sortear esta dificultad es la observación de que podemos deshacernos de
$E_{out}(h)$ totalmente ya que la desviación entre $E_{in}(h)$ y $E_{out}(h)$
puede ser capturada por las desviaciones entre dos errores dentro de la muestra:
$E_{in}$, el error dentro de la muestra original, y el mismo error en otro
conjunto de datos independiente, tal como se expone en~\ref{lema:A2}
Es una idea similar a cuando usamos un conjunto de test para estimar $E_{out}$.
Esto nos lleva a dos simplificaciones:

\begin{enumerate}
    \item
    El supremo de las desviaciones sobre infinitas $h \in \mathcal{H}$ pueden ser
    reducidas considerando sólo las dicotomías implementables por $\mathcal{H}$
    en los dos conjuntos de datos independientes. Aquí es donde la función
    de crecimiento $m_{\mathcal{H}}(2N)$ entra en escena~\ref{lema:A3}.
    \item
    La desviación entre dos errores dentro de la muestra \emph{independientes}
    es ``fácil'' de analizar comparado con $E_{in}$ y $E_{out}$,\ref{lema:A4}.
\end{enumerate}

La combinación de~\ref{lema:A2},~\ref{lema:A3} y~\ref{lema:A4} prueba~\ref{th:A1}.

\section{Relacionando el error de generalización con las desviaciones dentro
de la muestra}

Se introduce un segundo conjunto de datos $\mathcal{D}'$, independiente de
$\mathcal{D}$ pero acorde a la misma distribución $P(x,y)$. Este segundo conjunto
de datos se conocerá como conjunto de datos \emph{fantasma}, ya que realmente
no existe, sólo es una herramienta utilizada en el análisis. Se espera poder
acotar el término $\mathbb{P} \left[| E_{in} - E_{out} | \;\text{es grande} \right]$
por otro término $\mathbb{P} \left[| E_{in} - E'_{in} | \;\text{es grande} \right]$
el cual es más fácil de analizar.

La intuición detrás de la demostración formal es la siguiente. Para cada hipótesis
$h$, como $\mathcal{D}'$ está muestreado de manera independiente desde $P(x,y)$,
la desigualdad de Hoeffding garantiza que $E'_{in}(h) \approx E_{out}(h)$ con
una gran probabilidad. Cuando $| E_{in}(h) - E_{out}(h) |$ es grande, con una
alta probabilidad $| E_{in}(h) - E'_{in}(h) |$ también lo es. Entonces,
$\mathbb{P} \left[| E_{in}(h) - E_{out}(h) | \;\text{es grande} \right]$ puede acotarse aproximadamente por
$\mathbb{P} \left[| E_{in}(h) - E'_{in}(h) | \;\text{es grande} \right]$.


Se intenta acotar la probabilidad de que $E_{in}$ esté lejos $E_{out}$. Sea
$E'_{in}(h)$ el error ``dentro de la muestra'' para una hipótesis $h$ en $\mathcal{D}'$.
Se supone que $E_{in}$ está lejos de $E_{out}$, con alguna probabilidad
(igualmente $E'_{in}(h)$ está lejos de $E{out}$, con la misma probabilidad, ya
que $E_{in}$ y $E'_{in}$ están idénticamente distribuidas). Cuando N es grande,
la probabilidad es aproximadamente una Gaussiana sobre $E_{out}$. Las colas
representan los casos cuando $E_{in}$ está lejos de $E_{out}$.

En estos casos, $E'_{in}$ está lejos de $E_{in}$ la mitad del tiempo.Esto es,
$\mathbb{P} \left[| E_{in} - E_{out} | \;\text{es grande} \right]$
puede ser acotado aproximadamente por
$2   \mathbb{P} \left[| E_{in} - E'_{in} | \;\text{es grande} \right]$.

Este argumento hace intuir que las desviaciones entre $E_{in}$ y $E_{out}$ pueden
ser capturadas por las desviaciones entre $E_{in}$ y $E'_{in}$.
Este argumento puede ser extendido para múltiples hipótesis


\begin{lema}
\label{lema:A2}
    \begin{displaymath}
        \left(1-2e^{- \frac{1}{2} \epsilon^2 N} \right)
        \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right]
        \leq
        \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \right]
    \end{displaymath}
Donde la probabilidad del miembro de la derecha es sobre $\mathcal{D}$ y
$\mathcal{D}'$ en conjunto.
\end{lema}

\begin{proof}
    Podemos asumir que $\mathbb{P} \left[ \sup\limits_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] > 0$,
    ya que en otro caso no hay nada que probar.
    \begin{align}
        & \ \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \right] \nonumber \\
        \geq & \ \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \wedge
        \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \label{eq:A1} \\
        =& \ \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \times \nonumber \\
        & \ \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert
          \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \nonumber
    \end{align}
    La desigualdad~\ref{eq:A1} se da ya que
    $\mathbb{P}[\mathcal{B}_1] \geq \mathcal{P} \left[ \mathcal{B}_1 \ \wedge
    \ \mathcal{B}_2 \right]$ para cualesquiera dos eventos
    $\mathcal{B}_1$, $\mathcal{B}_2$.
    Ahora, se considera el último término:


    \begin{displaymath}
        \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert
          \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right]
    \end{displaymath}

    El evento sobre el que condicionamos es un conjunto de conjuntos de datos
    con probabilidad no cero. Se fija un conjunto de datos $\mathcal{D}$
    en este evento, y sea $h^*$ cualquier hipótesis que cumpla
    $ | E_{in}(h^*) - E_{out}(h^*) | > \epsilon $. Una de estas hipótesis debe
    existir ya que $\mathcal{D}$ está en el evento o es con lo que condicionamos.
    La hipótesis $h^*$ no depende de $\mathcal{D}'$, pero sí de $\mathcal{D}$.

    \begin{align}
          & \ \mathbb{P} \left[ \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2}
            \Big\vert
            \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \nonumber \\
    \geq & \ \mathbb{P} \left[ | E_{in}(h^*) - E'_{in}(h^*) | > \frac{\epsilon}{2} \Big\vert
            \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \label{eq:A2} \\
    \geq & \ \mathbb{P} \left[ | E'_{in}(h^*) - E_{out}(h^*) | \leq \frac{\epsilon}{2}
            \Big\vert
            \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \label{eq:A3} \\
    \geq & \ 1 - 2e^{-\frac{1}{2} \epsilon^2 N} \label{eq:A4}
    \end{align}


    \begin{enumerate}
        \item La desigualdad~\ref{eq:A2} se cumple porque el evento ``$| E_{in}(h^*) - E'_{in}(h^*) | > \frac{\epsilon}{2}$'' implica ``$\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2}$''.
        \item La desigualdad~\ref{eq:A3} se cumple porque los eventos ``$| E'_{in}(h^*) - E_{out}(h^*) | \leq \frac{\epsilon}{2}$'' y ``$| E_{in}(h^*) - E_{out}(h^*) | > \epsilon$'' (que se cumple) implican ``$| E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2}$''.
        \item La desigualdad~\ref{eq:A4} se cumple porque $h^*$ es fijo con respecto a $\mathcal{D}'$
        y podemos aplicar la desigualdad de Hoeffding a $\mathbb{P} \left[| E'_{in}(h^*) - E_{out}(h^*) |
        \leq \frac{\epsilon}{2} \right]$.
    \end{enumerate}
    Es claro que la desigualdad de Hoeffding se aplica a $\mathbb{P} \left[| E'_{in}(h^*) - E_{out}(h^*) |
    \leq \frac{\epsilon}{2} \right]$ para cada $h^*$, ya que es fijo con respecto a $\mathcal{D}'$.
    Además también se aplica a cada media ponderada de $\mathbb{P} \left[| E'_{in}(h^*) - E_{out}(h^*) | \leq \frac{\epsilon}{2} \right]$ basada en $h^*$. Por último, como $h^*$ depende de una particular
    $\mathcal{D}$,tomamos la media ponderada sobre todos los $\mathcal{D}$ en el evento

    \begin{displaymath}
        \sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon
    \end{displaymath}

    que es sobre el que condicionamos, wheredonde los pesos vienen de la probabilidad
    de una $\mathcal{D}$ en particular. Como la cota sirve para cada $\mathcal{D}$
    en este evento, se mantiene para la media ponderada.
  \end{proof}

  Aclarar que se puede asumir $e^{- \frac{1}{2} \epsilon^2N} < \frac{1}{4}$,
  ya que en otro caso la cota dada en~\ref{th:A1} es clara.
  En este caso, $1 - 2e^{- \frac{1}{2} \epsilon^2N} > \frac{1}{2}$, por lo que
  el lema implica

  \begin{displaymath}
    \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E_{out}(h) | > \epsilon \right] \leq
    2 \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \right].
  \end{displaymath}




\section{Acotando la desviación del peor caso usando la función de crecimiento}


Ahora que se ha relacionado el error de generalización con las desviaciones entre
los errores dentro de la muestra, se puede trabajar con $\mathcal{H}$ restringido
a dos conjuntos de datos de tamaño $N$ cada uno, en vez de tamaño infinito.
Se quiere acotar

\begin{displaymath}
  \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \right]
\end{displaymath}

donde la probabilidad es sobre la distribución conjunta de los conjuntos de datos
$\mathcal{D}$ y $\mathcal{D}'$. Una forma equivalente de muestrear dos conjuntos
de datos $\mathcal{D}$ y $\mathcal{D}'$ es primero muestrear un conjunto de datos
$\mathcal{S}$ de tamaño $2N$, y luego particionar aleatoriamente $\mathcal{S}$
en $\mathcal{D}$ y $\mathcal{D}'$. Este muestreo aleatorio, sin
reemplazamiento, tiene $N$ muestras de $\mathcal{S}$ para $\mathcal{D}$, dejando
el resto para $\mathcal{D}'$. Dado el conjunto de datos unido $\mathcal{S}$, sea

\begin{displaymath}
    \mathbb{P} \left[\sup_{h \in \mathcal{D}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \
    \mathcal{S}  \right]
\end{displaymath}

la probabilidad de la desviación entre los dos errores dentro de la muestra,
donde la probabilidad se toma sobre las particiones aleatorias de $\mathcal{S}$
en $\mathcal{D}$ y $\mathcal{D}'$. Por la ley de la probabilidad total,
(con $\sum$ denotando suma o integral, según el caso),

\begin{align*}
    & \ \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | >
    \frac{\epsilon}{2} \right]  \\
    = & \ \sum_{\mathcal{S}} \mathbb{P} \left[\mathcal{S}  \right] \times
    \mathbb{P} \left[\sup_{h \in \mathcal{H}} |E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2}
    \Big\vert \mathcal{S}  \right]  \\
    \leq & \ \sup_{\mathcal{S}} \; \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2}
    \Big\vert \mathcal{S} \right].
\end{align*}

Sean $\mathcal{H}(\mathcal{S})$ las dicotomías que $\mathcal{H}$ puede implementar
en los puntos de $\mathcal{S}$. Por definición de la función de crecimiento,
$\mathcal{H}(\mathcal{S})$ no puede tener más de $m_{\mathcal{H}}(2N)$ dicotomías.
Se supone que tiene $M \leq m_{\mathcal{H}}(2N)$ dicotomías, notadas por $h_1,\ldots, h_M$.
Entonces,

\begin{displaymath}
    \sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{h} | = \sup_{h \in \{h_1,\ldots, h_M\}} | E_{in}(h) - E'_{in}(h) |.
\end{displaymath}

Además,

\begin{align}
   & \ \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \nonumber \\
    = & \ \mathbb{P} \left[\sup_{h \in \{h_1,\ldots, h_M\}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \nonumber \\
    \leq & \ \sum_{m=1}^M \mathbb{P} \left[| E_{in}(h_m) - E'_{in}(h_m) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \label{eq:A5} \\
    \leq & \ M \times \sup_{h \in \mathcal{H}} \mathbb{P} \left[ | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right], \label{eq:A6}
\end{align}

donde se ha usado la cota de la unión en~\ref{eq:A5}, y sobreestimando cada
término por el supremo sobre todas las posibles hipótesis para conseguir~\ref{eq:A6}.
Después, usando $M \leq m_{\mathcal{H}}(2N)$ y tomando el supremo sobre $\mathcal{S}$,
se ha probado:

\begin{lema}
\label{lema:A3}
   \begin{align}
     & \ \mathbb{P} \left[\sup_{h \in \mathcal{H}} | E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \nonumber \\ & \
    \leq m_{\mathcal{H}}(2N) \times \sup_{\mathcal{S}} \sup_{h \in \mathcal{H}} \mathbb{P} \left[| E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \nonumber,
   \end{align}
\end{lema}

donde la probabilidad del miembro de la izquierda es sobre la distribución
conjunta de $\mathcal{D}$ y $\mathcal{D}'$, y la probabilidad en el miembro de
la derecha es sobre particiones aleatorias de $\mathcal{S}$ en dos conjuntos
$\mathcal{D}$ y $\mathcal{D}'$.

La consecuencia más importante del lema~\ref{lema:A3} es que se ha sacado el
supremo de $h \in \mathcal{H}$ fuera de la probabilidad, a expensas del factor
extra de la función de crecimiento $m_{\mathcal{H}}(2N)$.

\section{Acotando la desviación entre errores dentro de la muestra}

Ahora se presenta el problema puramente combinatorio de acotar

\begin{displaymath}
    \sup_{\mathcal{S}} \sup_{h \in \mathcal{H}} \mathbb{P} \left[| E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right],
\end{displaymath}

que aparece en el lema~\ref{lema:A3}. Se probará el siguiente lema. Así, el
Teorema~\ref{th:A1} se prueba combinando los lemas~\ref{lema:A2},~\ref{lema:A3}
y~\ref{lema:A4} tomando $1 - 2e^{- \frac{1}{2} \epsilon^2 N} \geq \frac{1}{2}$,
que es el único caso que se tiene que considerar.

\begin{lema}
\label{lema:A4}
  Para cada h y cada S,
   \begin{displaymath}
       \mathbb{P} \left[| E_{in}(h) - E'_{in}(h) | > \frac{\epsilon}{2} \Big\vert \mathcal{S} \right] \leq 2e^{- \frac{1}{8} \epsilon^2 N},
   \end{displaymath}

   donde la probabilidad es sobre particiones aleatorias de $\mathcal{S}$
   en dos conjuntos $\mathcal{D}$ y $\mathcal{D}'$.

   \begin{proof}
      Para probar este resultado se usará un resultado previo de Hoeffding, para
      muestreos \emph{sin reemplazamiento}:
      \begin{lema}[Hoeffding, 1963]
      \label{lema:A5}
        Sea $\mathcal{A} = \{a_1,\ldots, a_{2N}$\} un conjunto de valores con
        $a_n \in [0,1]$, y sean $\mu = \frac{1}{2N} \sum_{n=1}^{2N} a_n$ su
        media. Sea $\mathcal{D} = \{z_1,\ldots, z_N\}$ una muestra de tamaño
        $N$, tomada desde $\mathcal{A}$ sin reemplazamiento. Entonces

        \begin{displaymath}
         \mathbb{P} \left[ \left| \frac{1}{N} \sum_{n=1}^N z_n - \mu \right| > \epsilon \right] \leq 2e^{-2 \epsilon^2 N}.
        \end{displaymath}
      \end{lema}

      Se aplica el lema~\ref{lema:A5} de esta manera. Para los $2N$ ejemplos en
      $\mathcal{S}$, sea $a_n = 1$ si $h(x_n) \neq y_n$ y $a_n = 0$ en otro caso.
      Los $\{a_n\}$ son los errores de $h$ en $\mathcal{S}$. Ahora se particiona
      aleatoriamente $\mathcal{S}$ en $\mathcal{D}$ y $\mathcal{D}'$ tomando $N$
      muestras de $\mathcal{S}$ sin reemplazamiento para obtener $\mathcal{D}$,
      y las $N$ restantes para $\mathcal{D}'$. Esto lleva a una muestra de tamaño
      $N$ de los $\{a_n\}$ para $\mathcal{D}$, muestreados uniformemente
      sin reemplazamiento. Por tanto,

      \begin{displaymath}
          E_{in}(h) = \frac{1}{N} \sum_{a_n \in \mathcal{D}} a_n, \ \text{y} \ E'_{in}(h) = \frac{1}{N} \sum_{a'_n \in \mathcal{D}'} a'_n.
      \end{displaymath}

     Como muestreamos sin reemplazamiento, $\mathcal{S} = \mathcal{D} \cup \mathcal{D}'$
     y $\mathcal{D} \cap \mathcal{D}' = \emptyset$, y por tanto

         \begin{displaymath}
             \mu = \frac{1}{2N} \sum_{n=1}^{2N} a_n = \frac{E_{in}(h) + E'_{in}(h)}{2}.
         \end{displaymath}
         Entonces es claro que $| E_{in} - \mu | > t \Leftrightarrow | E_{in} - E'_{in} | > 2t$.
         Por el lema~\ref{lema:A5},

         \begin{displaymath}
             \mathbb{P} \left[| E_{in}(h) - E'_{in}(h) | > 2t \right] \leq 2e^{-2t^2 N}.
         \end{displaymath}

         Sustituyendo $t = \frac{\epsilon}{4}$ ofrece el resultado deseado.

   \end{proof}
\end{lema}
