
Se introducen las herramientas matemáticas necesarias para la consecución de este trabajo.

<<setup05M, cache = FALSE, include = FALSE>>=
set.seed(1254221004)
read_chunk("../scripts/05_Desarrollo_Matematicas.R")
@

\subsection{Introducción al problema}
\label{subsec:introduccion}

Parafraseando a~\citep{Abu-Mostafa:2012:LD:2207825}, si a un niño pequeño se le pregunta si hay un árbol
en una foto, lo más probable es que dé la respuesta correcta. Si a uno algo mayor
se le pregunta ``¿Qué es un árbol'', posiblemente no sabrá contestar o su respuesta
no sea muy útil. Los seres humanos no han aprendido qué es un árbol con una definición
matemática, sino viendo árboles. Han aprendido desde los \emph{datos}.

Este tipo de aprendizaje puede ser modelado matemáticamente para solucionar una gran
cantidad de problemas en ciencia, ingeniería, economía, \ldots, etc. Esta rama de
las matemáticas se llama \emph{Aprendizaje Estadístico}.

Matemáticamente, el problema del aprendizaje es, dado un conjunto de puntos de entrada
$\mathcal{X} \in \mathbb{R}^d$, un espacio de salida $\mathcal{Y}$, una función objetivo
\textbf{desconocida} \ $f:\mathcal{X} \rightarrow \mathcal{Y}$ y $N$ muestras
$\big((x_1,y_1),\ldots,(x_N,y_N) \big)$
tal que $f(x_i) = y_i, \; i = 1,\ldots,N$,
se desea conseguir una función de un determinado espacio de hipótesis,
$g \in \mathcal{H}$ que se asemeje todo lo posible a la función objetivo
$f$, $g \approx f$.


En el aprendizaje se distinguen muchos problemas, siendo tres los más conocidos:
\begin{itemize}
  \item Regresión: Se busca una relación entre la variable dependiente y la
  independiente.
  \item Clasificación: Se busca una función que para una entrada, le asigne
  una etiqueta. En problemas binarios, esta etiqueta suele ser -1 o 1.
  \item Estimación de densidad: Similar a clasificación, pero el objetivo no
  es dar una clase, sino la probabilidad de pertenecer a dicha clase.
\end{itemize}


Un ejemplo de clasificación sería el siguiente:
sea $\mathcal{X} \subseteq [0,1] \times [0,1]$ e $\mathcal{Y} = \{+1,-1\}$ el espacio
de salida, el cual se puede considerar sí/no, $f$ la función objetivo.
Generando muestras aleatorias, un ejemplo sencillo sería el de la
figura~\ref{fig:dataExample}. En lenguaje natural, tenemos dos características
con las que se puede decidir si un correo electrónico entrante es spam, por ejemplo.
Se necesita una función $g$ que cumpla que $\forall x \in \mathcal{X}$, $g(x) = f(x)$.
Intuitivamente, en el caso de clasificación se busca una función que \emph{separe}
los conjuntos de datos con distintas etiquetas. ¿Existe un separador para este conjunto?
En este caso, \textbf{sí}, siendo además el más sencillo, un separador \emph{lineal}.
Así se puede ver en la figura~\ref{fig:classificationExample}.

<<randomLine, include = FALSE>>=
@

<<signOfPoint, include = FALSE>>=
@

<<dataExample, echo = FALSE, fig.cap="Datos de ejemplo">>=
@

<<classificationExample, echo = FALSE, fig.cap="Primer clasificador">>=
@

En este sencillo caso se puede deducir lo siguiente: en un lado de la recta obtenida
se tienen todos los datos con etiquetas $1$ y en el otro los que tienen etiquetas $-1$.
A los conjuntos de datos que cumplen esto los llamaremos \emph{linealmente separables}.
Este hecho no es lo usual en un problema de aprendizaje real. Ni siquiera se sabe
a priori si, ante nuevos datos, esta recta los seguirá separando correctamente.
Incluso, ante nuevos datos, el conjunto podría pasar a ser no linealmente separable.

\subsection{Factibilidad del aprendizaje}
\label{subsec:factibilidad}

El aprendizaje estadístico busca \emph{obtener información desde los datos},
utilizando para ello técnicas estadísticas. Se empezará estudiando una cuestión
que se ha planteado antes por encima: ante la llegada de nuevos datos, ¿se tiene
un modelo adecuado? ¿Es factible aprender?

\begin{definicion}
  Error dentro de la muestra ($E_{in}$). Es la fracción de $\mathcal{D}$ donde
  la función escogida $h$ y la objetivo $f$ difieren. $\chi(condicion)$ es la
  función indicadora: vale 1 si la condición es cierta y 0 si no lo es.

  \[
    E_{in}(h) = \frac{1}{N} \sum\limits_{n=1}^N \chi(h(x_n) \neq f(x_n))
  \]
\end{definicion}

\begin{definicion}
  Error fuera de la muestra ($E_{out}$). Es la probabilidad, basada en la distribución
  de $\mathcal{X}$, de que la función escogida $h$ y la objetivo $f$ difieren.

  \[
    E_{out}(h) = \mathbb{P}[h(\textbf{x}) \neq f(\textbf{x})]
  \]
\end{definicion}

Es claro que $E_{in}$ se comporta como un estimador de la esperanza de
$E_{out}$, puesto que esta última se comporta como una variable aletatoria
sobre la distribución de $\mathcal{X}$ y $E_{in}$ es sólo una muestra de lo que
vale para una función $h$ fija.

\begin{teorema}[\cite{10.2307/2282952}]
  Si $X_1, X_2, \ldots, X_n$ son independientes y $a_i \leq X_i \leq b_i \; (i=1,2,\ldots,n)$,
  entonces para $\epsilon>0$ se da:
  \[
    \mathbb{P}\left[ \bar{X} - \mathbb{E}[\bar{X}] \geq \epsilon \right] \leq
    e^{\frac{-2 n^2 \epsilon^2}{\sum\limits_{i=1}^n(b_i-a_i)^2}}
  \]
\end{teorema}

\begin{proof}
  Primero se expondrán algunos resultados necesarios.
  \begin{lema}
  \label{lema:cotaConvexa}
    Si $X$ es una variable aleatoria tal que $a \leq X_i \leq b$, entonces
    para cualquier número real $h$

    \begin{displaymath}
      \mathbb{E}[e^{hX}] \leq \frac{b-\mathbb{E}[X]}{b-a}e^{ha} + \frac{\mathbb{E}[X]-a}{b-a}e^{hb}
    \end{displaymath}
  \end{lema}

  \begin{proof}
    Como la función exponencial $e^{hX}$ es convexa, su gráfica está acotada
    superiormente por la línea recta que une $X=a$ y $X=b$.

    \begin{displaymath}
      e^{hX} \leq \frac{b-X}{b-a}e^{ha} + \frac{X-a}{b-a}e^{hb}
    \end{displaymath}

    La hipótesis queda probada por la linealidad de la esperanza.
  \end{proof}

  \begin{lema}
  \label{lema:17}
    Siendo $S = X_1 + X_2 + \cdots + X_n \; h \in \mathbb{R}$,
    \begin{displaymath}
      \mathbb{P}[\bar{X}-\mathbb{E}[\bar{X}] \geq t] =
      \mathbb{P}[S - \mathbb{E}[S] \geq nt] \leq
      \mathbb{E}[e^{h(S- \mathbb{E}[S]-nt)}]
    \end{displaymath}

    Además, si los sumandos de S son independientes,
    \begin{displaymath}
      \mathbb{E}[e^{h(S- \mathbb{E}[S]-nt)}] =
      e^{-hnt} \prod_{i=1}^n \mathbb{E}[e^{h(X_i - \mathbb{E}[X_i])}]
    \end{displaymath}
  \end{lema}



    Notaremos $\mu_i = \mathbb{E}[X_i]$. Por~\ref{lema:17},

  \begin{equation}
  \label{eq:Pleqe}
    \mathbb{P}[\bar{X}-\mathbb{E}[\bar{X}] \geq t]\leq e^{-hnt} \prod_{i=1}^n Ee^{h(X_i- \mu_i)}
  \end{equation}

  Por el lema~\ref{lema:cotaConvexa},

  \begin{equation}
  \label{eq:Eleq}
      \mathbb{E}[e^{h(X_i- \mu_i)}] \leq e^{-h \mu_i} \left(\frac{b_i- \mu_i}{b_i-a_i} e^{ha_i} + \frac{\mu_i-a_i}{b_i-a_i} \right) = e^{L(h_i)}
  \end{equation}

  donde

  \begin{displaymath}
      L(h_i) = -h_ip_i + \ln{(1-p_i + p_ie^{h_i})}
  \end{displaymath}

  \begin{displaymath}
      h_i = h(b_i-a_i), \ \ \ p_i = \frac{\mu_i-a_i}{b_i-a_i}.
  \end{displaymath}

  Las primeras dos derivadas de $L(h_i)$ son

  \begin{displaymath}
      L'(h_i) = -p_i + \frac{p_i}{(1-p_i)e^{-h_i} + p_i}
  \end{displaymath}

  \begin{displaymath}
      L''(h_i) = \frac{p_i(1-p_i)e^{-h_i}}{[(1-p_i)e^{-h_i}+ p_i]^2}
  \end{displaymath}

  La última proporción es del tipo $u(1-u)$ donde $0 \leq u \leq 1$. Por tanto
  $L''(h_i) \leq \frac{1}{4}$. Entonces, utilizando la fórmula de Taylor,

  \begin{displaymath}
      L(h_i) \leq L(0) + L'(0)h_i + \frac{1}{8}h_i^2 = \frac{1}{8}h_i^2 = \frac{1}{8}h_i^2 (b_i-a_i)^2
  \end{displaymath}

  Entonces por~\ref{eq:Eleq}

  \begin{displaymath}
      \mathbb{E}[e^{h(x_i- \mu_i)}] \leq e^{\frac{1}{8}h^2 (b_i-a_i)^2}
  \end{displaymath}

  y por~\ref{eq:Pleqe}

  \begin{equation}
  \label{eq:final}
      \mathbb{P}[\bar{X} - \mu \geq t] \leq e^ {-hnt + \frac{1}{8}h^2 \sum_{k=1}^n (b_i-a_i)}
  \end{equation}

  El miembro de la derecha de~\ref{eq:final} tiene un mínimo en $h=4nt/ \sum (b_i-a_i)^2$.
  Insertando este valor en~\ref{eq:final} se obtiene la desigualdad deseada.
\end{proof}

\begin{corolario}
  \label{cor:hoeffdingH}
  Si $X_1, X_2, \ldots, X_n$ son independientes y $0 \leq X_i \leq 1 \; (i=1,2,\ldots,n)$,
  entonces para $\epsilon>0$ y una función $h$ se da:
  \[
    \mathbb{P}\left[ | E_{in}(h) - E_{out}(h) | \geq \epsilon \right] \leq
    2 e^{-2 n \epsilon^2}
  \]
\end{corolario}

\begin{proof}
  Se toman $a_i = 0\; b_i = 1$. La cota con el valor absoluto es clara,
  ya que es claro que $\mathbb{P}[-a \geq t] \leq \mathbb{P}[a \geq t]\; t \geq 0$,
  además que
  \begin{displaymath}
    \mathbb{P}[|a| \geq t] = \mathbb{P}[-a \geq t] + \mathbb{P}[a \geq t]
  \end{displaymath}
\end{proof}

Todo lo anterior tiene una gran limitación: se está fijando una función $h$ de
antemano. Al fijarla previamente, esta probabilidad depende del conjunto
de datos aleatorio $\mathcal{D}$.

Consideremos un espacio $\mathcal{H}$ con un número finito de hipótesis.

\[
  \mathcal{H} = \{h_1, h_2, \ldots, h_M\}
\]

Cuando se aplica un algoritmo de aprendizaje sobre este conjunto $\mathcal{H}$,
la hipótesis final $g \in \mathcal{H}$ está basada en el conjunto de datos
$\mathcal{D}$, por lo que a la conclusión que se pretende llegar no es exactamente
la del corolario~\ref{cor:hoeffdingH}, sino que se precisa eliminar la hipótesis
\emph{para una función fija}.

\begin{corolario}
  \label{cor:hoeffdingG}
  Si $X_1, X_2, \ldots, X_n$ son independientes y $0 \leq X_i \leq 1 \; (i=1,2,\ldots,n)$,
  entonces para $\epsilon>0$ y una función $g \in \mathcal{H}$ con
  $|\mathcal{H}| = M$ se da:
  \[
    \mathbb{P}\left[ | E_{in}(g) - E_{out}(g) | \geq \epsilon \right] \leq
    2 M e^{-2 n \epsilon^2}
  \]
\end{corolario}

\begin{proof}
  Por como está definida $g$, se tiene que
  \begin{align*}
    |E_{in}(g) - E_{out}(g)| \geq \epsilon \Rightarrow & \; |E_{in}(h_1) - E_{out}(h_1)| \geq \epsilon \\
                              & \vee |E_{in}(h_2) - E_{out}(h_2)| \geq \epsilon \\
                              & \cdots \\
                              & \vee |E_{in}(h_M) - E_{out}(h_M)| \geq \epsilon
  \end{align*}
  Ahora usaremos dos reglas básicas en probabilidad:
  \begin{itemize}
    \item Si $\mathcal{E}_1 \Rightarrow \mathcal{E}_2$ son eventos, entonces
    $\mathbb{P}[\mathcal{E}_1] \leq \mathbb{P}[\mathcal{E}_2]$
    \item Si tenemos $\mathcal{E}_1, \mathcal{E}_2, \ldots, \mathcal{E}_M$ eventos,
    entonces $\mathbb{P}[\mathcal{E}_1 \vee \mathcal{E}_2 \vee \ldots \vee \mathcal{E}_M]
    \leq \mathbb{P}[\mathcal{E}_1] + \mathbb{P}[\mathcal{E}_2] + \ldots + \mathbb{P}[\mathcal{E}_M]$
  \end{itemize}
  Utilizando estas dos reglas es claro que
  \begin{align*}
    \mathbb{P}[|E_{in}(g) - E_{out}(g)| \geq \epsilon] \Rightarrow & \; \mathbb{P}[|E_{in}(h_1) - E_{out}(h_1)| \geq \epsilon \\
                              & \vee |E_{in}(h_2) - E_{out}(h_2)| \geq \epsilon \\
                              & \cdots \\
                              & \vee |E_{in}(h_M) - E_{out}(h_M)| \geq \epsilon ] \\
                              \leq & \sum\limits_{m=1}^M \mathbb{P}[|E_{in}(h_M) - E_{out}(h_M)| \geq \epsilon]
  \end{align*}
  Aplicando la desigualdad de Hoeffding del corolario~\ref{cor:hoeffdingH} a cada uno de los
  $M$ términos, se acota cada uno por $e^{-2N \epsilon^2}$. Sumando todo queda
  \[
  \mathbb{P}\left[ | E_{in}(g) - E_{out}(g) | \geq \epsilon \right] \leq
  2 M e^{-2 n \epsilon^2}
  \]
\end{proof}

Esta desigualdad tiene una gran importancia: si el conjunto de funciones hipótesis
$\mathcal{H}$ es finito, se puede afirmar que, probabilísticamente,
$E_{in}(g) \approx E_{out}(g)$, aunque sólo tiene sentido si el conjunto de
hipótesis es finito. Además, la cota es peor cuanto más grande es este espacio
de hipótesis.

Para que el aprendizaje sea factible, se deben cumplir dos cosas:
\begin{enumerate}
  \item $E_{out}(g) \approx E_{in}(g)$ \checkmark
  \item $E_{in}(g) \approx 0$
\end{enumerate}

Lo primero está probado para conjuntos de hipótesis finito. Lo segundo es un valor
conocido: una vez un algoritmo de aprendizaje nos da una función $g$, podemos
calcular su $E_{in}$ para evaluar su exactitud.

\subsection{Generalización: Teoría de Vapnik-Chervonenkis}
\label{subsec:generalizacion}
En esta sección se desarrollará una teoría matemática que caracteriza la distinción
entre entrenamiento y validación, además de discutir las implicaciones conceptuales
y prácticas de ambos.

\subsubsection{Teoría de Generalización}
\label{subsubsec:generalizacion}
El error fuera de la muestra $E_{out}$ mide la exactitud del modelo generado en
puntos que desconocemos, al contrario que el error dentro de la muestra $E_{in}$,
que se basa en el espacio de entrada $\mathcal{X}$. Expresa así el concepto de
\emph{error de entrenamiento}, por lo que se usarán indistintamente.

En esta sección se generalizará el análisis expuesto en el anterior capítulo.
Se ha discutido que no siempre el valor de $E_{in}$ se generaliza a un valor similar
de $E_{out}$.

\begin{definicion}[Error de generalización]
  Es la diferencia entre $E_{in}$ y $E_{out}$ en valor absoluto.
\end{definicion}

La desigualdad de Hoeffding proporciona una caracterización del error de
generalización con una cota probabilística.

\[
  \mathbb{P}\left[ |E_{in}(g) - E_{out}(g)| > \epsilon \right] \leq
  2 M e^{-2 n \epsilon^2}
\]

Se puede reescribir de esta manera, siendo cierta con una probabilidad de $1-\delta$

\begin{equation}
  \label{eq:generalization}
  E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{1}{2N} \ln \frac{2M}{\delta}} ,\; \delta \ \text{nivel de tolerancia.}
\end{equation}

\begin{proof}
  La desigualdad de Hoeffding dice que con probabilidad al menos $1-2Me^{-2N\epsilon^2}$,
  $|E_{out} - E_{in}| \leq \epsilon \Rightarrow E_{out} \leq E_{in} + \epsilon$.
  Se toma $\delta = 2Me^{-2N\epsilon^2}$, por lo que $\epsilon = \sqrt{\frac{1}{2N} \ln \frac{2M}{\delta}}$
\end{proof}

La otra vuelta de la desigualdad $|E_{out} - E_{in}| \leq \epsilon$ ofrece otro
dato importante, $E_{out}(h) \geq E_{in}(h) - \epsilon \ \forall h \in \mathcal{H}$.
Quiere decir que no se puede realizar un ajuste mucho mejor con otra función del
espacio de hipótesis con mayor $E_{in}$ que la que se ha escogido.

Ahora se necesita trabajar con la ecuación~\ref{eq:generalization} de manera que
el espacio de funciones de hipótesis pueda ser infinito. Si se trabaja tal cual
está, la cota diverge y no ofrecería información alguna.

Tal y como se calculó $M$ en el anterior capítulo, al utilizar las reglas de
probabilidad de supuso que los eventos, como poco, serían disjuntos. Pero esto
no pasa usualmente: los eventos suelen estar solapados, por lo que se debe caracterizar
$M$ de manera distinta para que cuando se trabaje con un espacio infinito, se pueda
obtener una cota finita usando~\ref{eq:generalization}. A esta caracterización de $M$
se la llamará \emph{número efectivo de hipótesis}. Supondremos funciones
binarias, por lo que cada $h \in \mathcal{H}$ lleva elementos de $\mathcal{X}$
a $\{-1,+1\}$.

\begin{definicion}
  Sea $x_1,\ldots,x_N \in \mathcal{X}$. Las dicotomías generadas por $\mathcal{H}$
  en esos puntos están definidas por

  \begin{equation}
    \mathcal{H}(x_1,\ldots,x_N) = \{ h(x_1),\ldots,h(x_N) \mid h \in \mathcal{H}  \}
  \end{equation}
\end{definicion}

\begin{definicion}
  La función de crecimiento está definida para un conjunto de hipótesis $\mathcal{H}$
  como
  \begin{equation*}
    m_{\mathcal{H}}(N) = \max_{x_1,\ldots,x_N \in \mathcal{X}} |\mathcal{H}(x_1,\ldots,x_N)|,\
    \text{donde |·| es la cardinalidad del conjunto}
  \end{equation*}
\end{definicion}

$m_{\mathcal{H}}(N)$ es el número máximo de dicotomías que puede generar $\mathcal{H}$
para cualesquiera N puntos. El valor máximo que puede tomar es $|\{-1,+1\}^N|$,
por lo que

\begin{equation*}
  m_{\mathcal{H}}(N) \leq 2^N
\end{equation*}

Si se da la igualdad, quiere decir que $\mathcal{H}$ puede generar todas las
posibles dicotomías en $x_1,\ldots,x_N$, y se dirá que $\mathcal{H}$ puede
\emph{separar} $x_1,\ldots,x_N$.

Para cada conjunto de hipótesis que se use, no es fácil calcular de manera exacta
$m_{\mathcal{H}}$, aunque realmente no hay que hacerlo. Como $m_{\mathcal{H}}$
está pensado para sustituir a $M$ en~\ref{eq:generalization}, sólo hay que buscar
una cota superior, así la desigualdad seguirá siendo cierta.

\begin{definicion}
  Si para un determinado dataset de tamaño $k$ $\mathcal{H}$ no puede
  \emph{separarlo}, entonces $k$ es un punto de ruptura para $\mathcal{H}$.
\end{definicion}

Si $k$ es un punto de ruptura, entonces $m_{\mathcal{H}} < 2^k$. Es claro que
para un separador lineal como el de~\ref{fig:classificationExample}, $k=4$.

\begin{ejemplo}
  Tomamos un separador lineal, y $k=3$.
  <<breakExample, echo = FALSE, fig.pos = '!hbt', fig.width = 3, fig.height = 3, fig.cap = 'Ejemplo de dicotomías separables para un separador lineal'>>=
  @

  Como se puede apreciar en la figura~\ref{fig:breakExample}, cualquier dicotomía
  sobre tres puntos no alineados pueden ser \emph{separados} por un separador lineal.


  <<anotherbreakExample, echo = FALSE, fig.pos = '!hbt', fig.width = 3, fig.height = 3, fig.cap = 'Ejemplo de una dicotomía no separable para un separador lineal'>>=
  @

  En cambio, con $k=4$, es fácil ver que no se pueden obtener las $2^4=16$
  dicotomías necesarias para que $k=4$ no sea punto de ruptura. Se puede ver
  en la figura~\ref{fig:anotherbreakExample} que no se pueden clasificar
  correctamente todas las dicotomías de este conjunto.
\end{ejemplo}

Entonces, utilizando este concepto, se va a acotar la función de crecimiento
para cualquier valor de $N$, ya que el hecho de tener punto de ruptura $k$ ofrece
restricciones sobre el número de dicotomías que puede \emph{separar} $\mathcal{H}$
para un $k' > k$.

Lo más importante acerca de las funciones de crecimiento es que si tienen puntos
de ruptura, śe puede acotar por un polinomio basado en el punto de ruptura.

Al estar acotado por un polinomio, la derecha de la
ecuación~\ref{eq:generalization} tenderá a 0 cuando $N \rightarrow \infty$.

\begin{teorema}
  \label{th:boundMH}
  Si $m_{\mathcal{H}}(k) < 2^k$ para algún $k$, entonces
  \begin{equation}
    m_{\mathcal{H}}(N) \leq \sum_{i=0}^{k-1} \dbinom{N}{i} \ \forall N \in \mathbb{N}
  \end{equation}
  El miembro de la derecha es un polinomio en $N$ de grado $k-1$.
\end{teorema}

La gran implicación de este teorema es que si $\mathcal{H}$ tiene algún punto
de ruptura, se puede asegurar una buena generalización.

\subsubsection{La dimensión de Vapnik-Chervonenkis}
\label{subsubsec:VCdim}

Se ha conseguido una cota de la función de crecimiento en términos de cualquier
punto de ruptura. A más pequeño punto de ruptura, mejor es la cota. Ahora se
define un concepto que caracteriza a esta función.

\begin{definicion}
  La dimensión de Vapnik-Chervonenkis de un conjunto de hipótesis $\mathcal{H}$,
  denotada por $d_{VC}(\mathcal{H})$, o simplemente $d_{VC}$, es el mayor valor
  de $N$ para el que $m_{\mathcal{H}}(N) = 2^N$. Si ocurre para todo $N$,
  entonces $d_{VC}(\mathcal{H})=\infty$.
\end{definicion}

Utilizando este punto de ruptura, se puede reescribir el teorema~\ref{th:boundMH}
como sigue:

\begin{corolario}
  \label{cor:cotaComb}
  Se puede acotar la función de crecimiento de $\mathcal{H}$ como sigue:
  \begin{equation}
    m_{\mathcal{H}}(N) \leq \sum_{i=0}^{d_{VC}} \dbinom{N}{i} \ \forall N \in \mathbb{N}
  \end{equation}
\end{corolario}


\begin{proof}
  Es claro que si $d_{VC}$ es la dimensión de VC de $\mathcal{H}$, entonces
  $k=d_{VC}+1$ es un punto de ruptura para $m_{\mathcal{H}}$, por definición.
\end{proof}

Así, la dimensión VC es el orden del polinomio que acota la función de crecimiento.
También es lo mejor que se puede hacer con este razonamiento, ya que no hay puntos
de ruptura menores a $d_{VC}+1$. Se reescribe la cota para que sea de más fácil
manejo.

\begin{corolario}
  La función de crecimiento $m_{\mathcal{H}}(N)$ se puede acotar de manera más
  simple como

  \begin{equation}
    \label{eq:polBound}
    m_{\mathcal{H}}(N) \leq N^{d_{VC}} + 1
  \end{equation}
\end{corolario}

\begin{proof}
  Para demostrar este resultado se utilizará un lema previo.
  \begin{lema}
    \label{lema:cotaComb}
    En combinatoria, se da esta cota:
    \begin{equation}
      \sum_{i=0}^{D} \dbinom{N}{i} \leq N^{D} + 1 \ \forall N,D \in \mathbb{N}
    \end{equation}
  \end{lema}

  \begin{proof}
    Se realizará por inducción sobre $D$.

    \underline{$D = 1$}: Es claro que se cumple.

    \begin{displaymath}
      \dbinom{N}{0} + \dbinom{N}{1} \leq N^1 + 1 \Leftrightarrow N+1 \leq N+1
      \ \forall N \in \mathbb{N} \ \checkmark
    \end{displaymath}

    \underline{$D \overset{?}{\Rightarrow} D+1$}:

    \begin{displaymath}
      \sum\limits_{i=0}^{D+1} \dbinom{N}{i} \leq N^{D+1} + 1 \Leftrightarrow
      \sum_{i=0}^{D} \dbinom{N}{i} + \dbinom{N}{D+1} \leq N^{D+1} + 1
    \end{displaymath}

    Esto es implicado por

    \begin{displaymath}
      N^{D} + 1 + \dbinom{N}{D+1} \leq N^{D+1} + 1,
    \end{displaymath}

    debido a hipótesis de inducción y a la transitividad de la relación de orden
    en los números reales. Se resuelve:

    \begin{align*}
      & N^{D}  + \dbinom{N}{D+1} \leq N^{D+1} \Leftrightarrow
      \dbinom{N}{D+1} \leq N^D (N-1)
      \Leftrightarrow \\
      & \frac{N \cancel{(N-1)} (N-2) \cdots (N-D)}{(D+1)!}
      \leq N^D \cancel{(N-1)} \Leftrightarrow \\
      & \frac{\overbrace{N (N-2) \cdots (N-D)}^{\text{Hay D multiplicandos, luego }\leq N^D}}{(D+1)!}
      \leq N^D \ \forall N \in \mathbb{N} \ \checkmark
    \end{align*}

    Queda entonces demostrado

    \begin{equation*}
      \sum_{i=0}^{D} \dbinom{N}{i} \leq N^{D} + 1 \ \forall N,D \in \mathbb{N}
    \end{equation*}

  \end{proof}
  Para demostrar este resultado no hay más que aplicar el lema~\ref{lema:cotaComb}
  y el corolario~\ref{cor:cotaComb}.

  \begin{displaymath}
    m_{\mathcal{H}}(N) \leq \sum_{i=0}^{d_{VC}} \dbinom{N}{i} \leq N^{d_{VC}} + 1
  \end{displaymath}


\end{proof}

El último paso para llegar a la cota de generalización apropiada es sustituir
el número efectivo de hipótesis por el número de hipótesis. Aunque para eso
se debe ajustar un poco la cota.

\begin{teorema}[Cota de generalización de Vapnik-Chervonenkis]
  \label{th:VCBound}
  Para una función binaria \textbf{f}, un conjunto de hipótesis $\mathcal{H}$,
  cualquier algoritmo de aprendizaje $\mathcal{A}$ y cualquier distribución
  de probabilidad P, se da:

  \begin{equation}
    \label{eq:VCBound}
    E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{8}{N} \ln \frac{4m_{\mathcal{H}}(2N)}{\delta}} ,\; \delta \ \text{nivel de tolerancia.}
  \end{equation}
  con probabilidad $1-\delta$.
\end{teorema}

La demostración de este teorema es sumamente técnica, y se presenta en el
apéndice~\ref{apendice:demVC}

Este teorema es el más importante en la teoría del aprendizaje.
Con datos suficientes, para cada hipótesis en un espacio infinito con
$d_{VC}$ finita, generalizará bien desde $E_{in}$ hasta $E_{out}$, donde el
error convergerá a 0.

Así, se establece la factibilidad del aprendizaje con infinitos conjuntos de
hipótesis. Esto lleva a preguntarse si un resultado tan general actúa bien
sobre casos muy particulares.

La cota de VC tiene algunos problemas, principalmente tres:

\begin{enumerate}
  \item La desigualdad de Hoeffding básica que se ha usado no es muy fuerte.
  Ofrece la misma cota tanto si $E_{out}$ es cercano a 0 o a 0.5, siendo
  la varianza de $E_{in}$ muy diferente en ambos casos. Utilizar una misma cota
  con ambos casos puede presentar un déficit.
  \item Utilizar la función de crecimiento con $N$ puntos sin importarnos cuáles
  son nos da una estimación del peor caso, lo que hace que la cota no dependa
  de la distribución de los datos, siendo una cota menos fuerte.
  \item Acotar la función de crecimiento con un polinomio de orden $d_{VC}$ como
  se ha hecho en~\ref{eq:polBound} también afecta a la finura de la cota.
\end{enumerate}

Por estas razones se suele usar esta cota como una guía para la generalización,
no como algo cerrado. Este análisis establece la factibilidad del aprendizaje para
conjuntos de hipótesis infinitos, que es lo que se suele dar en la práctica.
Además, también se usa para comparar el rendimiento de varios modelos, aunque
esto está fuera del ámbito matemático y es algo más empírico.

\subsubsection{Complejidad de la muestra}
\label{subsubsec:complejidadMuestra}
La complejidad de la muestra denota cuántos ejemplos de entrenamiento $N$ son
necesarios para conseguir un determinado rendimiento en generalización. El
rendimiento está especificado con dos parámetros, $\epsilon$ y $\delta$.
$\epsilon$ determina el error de generalización permitido y $\delta$ determina
la tolerancia, la confianza de este error.

Podemos usar~\ref{eq:VCBound} para estimar la complejidad de la muestra para un
modelo de aprendizaje. Fijamos $\delta > 0$, y se quiere conseguir un error de
como mucho $\epsilon$.

\begin{displaymath}
  \sqrt{\frac{8}{N} \ln{\frac{4 m_{\mathcal{H}}(2N)}{\delta}}} \leq \epsilon
  \Leftrightarrow N \geq \frac{8}{\epsilon^2} \ln{\frac{4 m_{\mathcal{H}}(2N)}{\delta}}
  \overset{\ref{eq:polBound}}{\Rightarrow} N \geq \frac{8}{\epsilon^2} \ln{\frac{4 ((2N)^{d_{VC}}+1)}{\delta}}
\end{displaymath}

Con esta desigualdad se puede calcular $N$ con cualquier método numérico iterativo.
Lo más sencillo es tomar una primera aproximación $N$ en el miembro de la derecha,
realizar los cálculos y tomar dicho resultado como $N$ nuevamente. Después de
un par de pasos converge.

\subsubsection{Complejidad del modelo}
\label{subsubsec:complejidad}
A veces, se tiene un conjunto de datos $\mathcal{D}$ fijo, por lo que $N$
también lo está. Entonces la pregunta relevante es cuánto rendimiento se puede
esperar con dicho $N$. Utilizando la cota~\ref{eq:VCBound}:

\begin{displaymath}
  E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{8}{N} \ln \frac{4m_{\mathcal{H}}(2N)}{\delta}}
  \overset{\ref{eq:polBound}}{\Rightarrow} E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{8}{N} \ln{\frac{4 ((2N)^{d_{VC}}+1)}{\delta}}}
\end{displaymath}

Con esta expresión se puede acotar el $E_{out}$ según $E_{in}$ y \emph{algo}
que se puede calcular fácilmente.

Se puede ver dicho \emph{algo} como $\Omega(N,\mathcal{H},\delta)$, un término
que aumenta cuando la $d_{VC}(\mathcal{H})$ aumenta. Esta es la penalización
por la complejidad del modelo: $E_{out}$ es mayor cuanto más complejo es el
conjunto de hipótesis $\mathcal{H}$, mayor cuando más confianza queramos
($\delta$ bajo), y más bajo cuantos más ejemplos existan.

La conclusión es que se debe establecer un equilibrio, ya que un modelo complejo
reducirá el $E_{in}$ pero hará más grande $\Omega(N,\mathcal{H},\delta)$.

\subsubsection{El conjunto de test}
\label{subsubsec:testSet}
Cuando se pretende estimar con más precisión el $E_{out}$ se suele utilizar
un \emph{conjunto de test}. Es un subconjunto de $\mathcal{X}$ que no se utiliza
para entrenamiento. La hipótesis final $g$ se evalúa con dicho conjunto y
producirá un error $E_{test}$, que servirá como estimación de $E_{out}$.
Se puede estudiar lo bien que generaliza $E_{test}$ sobre $E_{out}$ con lo estudiado
previamente.

El número efectivo de hipótesis que importan en el comportamiento de
generalización de $E_{test}$ es 1, ya que sólo hay una hipótesis elegida además
de no cambiar ésta según el conjunto de test. Esto implica que se puede
utilizar la desigualdad de Hoeffding con una sola hipótesis. Así se tiene una
cota más fina que con~\ref{eq:VCBound}.

Un aspecto importante del conjunto de test es que no está sesgado. El conjunto
de entrenamiento tiene un sesgo ya que para elegir una hipótesis, un algoritmo
de aprendizaje buscará una hipótesis que se comporte bien dentro.
La cota~\ref{eq:VCBound} tiene este sesgo en cuenta implícitamente, lo que
lleva a un gran error. Además, tener un conjunto de test implica reducir el
número de muestras de entrenamiento, por lo que el $E_{in}$ será mayor.

\subsubsection{Equilibrio entre Aproximación y Generalización}
\label{subsubsec:equilibrio}

Con el análisis VC se ha mostrado que debe existir un equilibrio entre una buena
aproximación de $f$ en la muestra usando un modelo demasiado complejo, o una buena
generalización usando un modelo más simple. Se puede analizar este equilibrio
de otra manera, la cual es muy apropiada para medidas de error cuadráticas,
al contrario que el error binario usado en el análisis VC. En vez de acotar
$E_{out}$ por $E_{in}$ y un término de penalización $\Omega$, se descompone
$E_{out}$ en dos términos distintos: \emph{sesgo} y \emph{varianza}.

Para un conjunto de datos $\mathcal{D}$, el $E_{out}$ utilizando el error
cuadrático medio es, siendo $\mathbb{E}_{x}$ la esperanza con respecto a x
(basada en la distribución de probabilidad del espacio de entrada $\mathcal{X}$),

\begin{equation}
  \label{eq:EoutMSE}
  E_{out}(g^{(D)}(x)) = \mathbb{E}_{x}[(g^{(D)}(x) - f(x))^2]
\end{equation}

Es importante para el análisis que se haga énfasis en que la función escogida
$g$ depende del conjunto de datos $\mathcal{D}$. El primer paso es quitar la
dependencia de un conjunto de datos $\mathcal{D}$ tomando la esperanza
con respecto a todos los conjuntos de datos. Así se obtendrá un error fuera
de la muestra esperado para el modelo de aprendizaje que no depende de un
conjunto de datos particular. Las cuentas se realizarán añadiendo ruido
(de media 0 y varianza $\sigma^2$)
a la función objetivo, para ver el caso general.

\begin{displaymath}
\mathbb{E}_{\mathcal{D}}\left[E_{out}\left(g^D(x)\right) \right] = \mathbb{E}_{\mathcal{D}} \left[\mathbb{E}_x \left[ \left(g^D(x) - y(x) \right)^ 2  \right] \right] = \mathbb{E}_{\mathcal{D}} \left[\mathbb{E}_x \left[ \left(g^D(x) - f(x) - \epsilon(x) \right)^ 2  \right] \right]
\end{displaymath}

\begin{displaymath}
\mathbb{E}_{\mathcal{D}} \left[\mathbb{E}_x \left[ \left(g^D(x) - f(x) - \epsilon(x) \right)^ 2  \right] \right] = \mathbb{E}_x \left[\mathbb{E}_{\mathcal{D}} \left[ \left(g^D(x) - f(x) - \epsilon(x) \right)^ 2  \right] \right]
\end{displaymath}

Desarrollando el trinomio queda:

\begin{displaymath}
\mathbb{E}_x \left[\mathbb{E}_{\mathcal{D}} \left[ g^D(x)^2 + f(x)^2 + \epsilon(x)^2 - 2f(x)g^D(x) - 2g^D(x)\epsilon(x) - 2f(x)\epsilon(x) \right] \right]
\end{displaymath}

El término $\mathbb{E}_{\mathcal{D}} \left[ g^D(x) \right]$ provee una
``función media'', que se denota por $\bar{g}(x)$.

Ahora sumamos y restamos $\bar{g}(x)^2$ para obtener las expresiones para $bias(x)$ y $var(x)$.


\begin{displaymath}
\mathbb{E}_x \left[\mathbb{E}_{\mathcal{D}} \left[ g^D(x)^2 - \bar{g}(x)^2 + \bar{g}(x)^2 + f(x)^2 + \epsilon(x)^2 - 2f(x)g^D(x) - 2g^D(x)\epsilon(x) - 2f(x)\epsilon(x) \right] \right]
\end{displaymath}

Usamos la linealidad de la esperanza y reordenamos para obtener dichas expresiones. La varianza es la indicada porque el doble producto de que debe salir del binomio al cuadrado es $0$, ya que $\bar{g}(x)$ no depende de $\mathcal{D}$.

\begin{align*}
\mathbb{E}_x \Big[ & \underbrace{\mathbb{E}_{\mathcal{D}} \left[ g^D(x)^2 \right] - \bar{g}(x)^2}_{var(x)} + \underbrace{\bar{g}(x)^2 - 2f(x) \underbrace{\mathbb{E}_{\mathcal{D}}  \left[g^D(x) \right]}_{\bar{g}(x)} + f(x)^2}_{bias(x)}  - 2\epsilon(x) \underbrace{\mathbb{E}_{\mathcal{D}}  \left[g^D(x) \right]}_{\bar{g}(x)} \\
& - 2f(x)\epsilon(x) + \epsilon(x)^2 \Big]
\end{align*}

Entonces nos queda

\begin{displaymath}
\mathbb{E}_{\mathcal{D}} [E_{out}(g^D(x))] = \mathbb{E}_x [var(x) + bias(x) - 2 \epsilon(x) \bar{g}(x) - 2f(x)\epsilon(x) + \epsilon(x)^2 ]
\end{displaymath}

Ahora usamos de nuevo la linealidad de la esperanza. Además, destacar que $f(x)$ y $\epsilon(x)$ son independientes, luego la esperanza del producto es el producto de las esperanzas.

\begin{displaymath}
\mathbb{E}_{\mathcal{D}}[E_{out}(g^D(x))] = var + bias -2 \underbrace{\mathbb{E}_x \left[ \epsilon(x) \right]}_{0} \mathbb{E}_x \left[\bar{g}(x) \right] - 2 \mathbb{E}_x \left[ f(x) \right] \underbrace{\mathbb{E}_x \left[ \epsilon(x) \right]}_{0} + \underbrace{\mathbb{E}_x \left[\epsilon(x)^2 \right]}_{\sigma^2}
\end{displaymath}

Con esto, ya lo tenemos: por hipótesis, sabemos que la media del ruido es 0 y la varianza $\sigma^2$, luego nos queda la expresión que buscábamos:

\begin{displaymath}
\mathbb{E}_{\mathcal{D}}[E_{out}(g^D(x))] = var + bias + \sigma^2
\end{displaymath}

En resumen:
\begin{itemize}
  \item $bias(x) = (\bar{g}(x) - f(x))^2$, el sesgo, mide la desviación de la función media con respecto a la función objetivo
  \item $var(x) = \mathbb{E}_{\mathcal{D}} \left[ g^D(x)^2 \right] - \bar{g}(x)^2$, la varianza, mide la variación de la hipótesis final dependiendo del conjunto de datos.
\end{itemize}

El equilibrio aproximación-generalización viene implícito en esta descomposición.
Si tenemos conjunto de hipótesis simple (una solo, para que quede más claro el ejemplo),
la hipótesis final y la función media serán la misma para cualquier conjunto de
datos, luego $var = 0$. El sesgo dependerá solamente de la función objetivo $f$.
Al haber una sola, lo más probable es que el sesgo sea muy alto.

En un conjunto de hipótesis grande, diferentes conjuntos de datos producirán
distintas hipótesis que se acerquen a $f$, luego el sesgo es aproximadamente 0,
ya que la función media estará cerca de $f$. La varianza es bastante grande al
variar mucho la función media con respecto a cada una de las generadas con el
conjunto de datos. También se puede ver como la inestabilidad del modelo, ya
que si una función generada es muy distinta a otra, querrá decir que se
comportará muy distinto en algunos de los datos.

En general, calcular estos valores no es fácil y no se suele hacer, por lo que
esta herramienta es puramente conceptual para el desarrollo de modelos. Cuando
se considera este enfoque, se suelen intentar dos cosas:

\begin{itemize}
  \item Bajar la varianza sin subir mucho el sesgo: Se utilizan técnicas
  generales como la regularización, que se estudia en el
  capítulo~\ref{subsec:sobreajuste}.
  \item Bajar el sesgo sin subir mucho la varianza: Requiere de mucha
  información para seleccionar el modelo $\mathcal{H}$ en función de $f$, por
  lo que es específico para cada aplicación.
\end{itemize}


\subsection{Sobreajuste}
\label{subsec:sobreajuste}

El sobreajuste es el hecho que se da cuando se ajustan los datos más de lo que
debieran. La principal causa es elegir una hipótesis con el menos $E_{in}$
y darse cuenta que su $E_{out}$ es mucho mayor, dando a entender que el error
dentro de la muestra no está siendo un buen guía para el aprendizaje.

Con un ejemplo se verá más claro. La función objetivo es una parábola, pero
los datos obtenidos tenían ruido. Al tener 5 datos, se puede pensar que lo mejor
es aprender usando polinomios de grado 4, ya que podrá ajustarlo perfectamente
(se estaría interpolando). Pero puede pasar lo que se ve en la
figura~\ref{fig:overfitting}.

<<overfitting, echo = FALSE, fig.cap = "Ejemplo de sobreajuste">>=
options(warn=-1)
set.seed(43556457)

f <- function(x) {x^2 + 5}

x <- c(1,2,5,10,12)
y <- sapply(x, f)

noise <- runif(n = 5, min = -20, max = 20)
df  <- data.frame(X = x,
                  Y = y + noise)

gg <- ggplot(df, aes(x = X, y = Y)) + geom_point(aes(size = "Data")) +
        stat_function(fun = f,aes(colour = "Target")) +
        theme(legend.title = element_blank())
gg + geom_smooth(method = "lm", formula = y ~ poly(x, 4), size = 1, se = F, aes(colour = "Fit"))
@

Se estaría atacando el ruido para ajustarlo totalmente, perdiendo así toda la
generalidad de la función objetivo.

El ruido se puede clasificar en dos tipos:

\begin{itemize}
  \item Ruido estocástico: Es el ruido presente en el anterior ejemplo, el
  ruido aleatorio. No puede ser modelado.
  \item Ruido determinista: Es la porción de función que la función hipótesis
  no puede ajustar de la objetivo. Es una parte de la función objetivo que no
  puede ser modelada.
\end{itemize}

Si un algoritmo de aprendizaje intenta ajustar el ruido, se producirá
sobreajuste. El algoritmo usará los grados de libertad que tenga para ajustarlo.

La descomposición bias-varianza estudiada en~\ref{subsubsec:equilibrio}
es una herramienta para entender cómo afecta el ruido al modelado.

\begin{displaymath}
\mathbb{E}_{\mathcal{D}}[E_{out}(g^D(x))] = \sigma^2 + bias + var
\end{displaymath}

Los primeros dos términos indican el impacto del ruido estocástico y determinan
el ruido determinista. La varianza del ruido estocástico es $\sigma^2$ y el
sesgo está directamente relacionado con el ruido determinista, al capturar
lo lejos que está de modelar $f$. La varianza está modificada por ambos
tipos de ruido, modelando la susceptibilidad del modelo a ser lastrado por
el ruido.

\subsubsection{Regularización}
\label{subsubsec:regularizacion}

La primera herramienta para intentar reducir el sobreajuste es la
regularización. Restringe al algoritmo de apendizaje a mejorar el $E_{out}$,
sobre todo cuando hay ruido presente. Un ejemplo se ve en la
figura~\ref{fig:firstRegularizer}.

<<firstRegularizer, echo = FALSE, fig.cap = "Ejemplo de regularización">>=
options(warn=-1)
gg + geom_smooth(method = "lm", formula = y ~ poly(x, 2), size = 1, se = F, aes(colour = "Fit"))
@

Muchos de los métodos de regularización existentes son heurísticos: no existe
un algoritmo perfecto para tratar el sobreajuste. Utilizando la cota de VC
tal como se estudió en~\ref{subsubsec:complejidad}, se puede acotar el $E_{out}$
por el $E_{in}$ y una penalización por el modelo.

\begin{displaymath}
  E_{out}(g) \leq E_{in}(g) + \Omega(\mathcal{H})
\end{displaymath}

Así que la clave está no en minimizar $E_{in}$, sino la combinación de éste
con la penalización $\Omega(\mathcal{H})$. Cada método tiene una manera de
medir esta penalización, haciendo que unos se comporten mejor que otros en
determinados problemas.

\subsubsection{Validación}
\label{subsubsec:validacion}

En esta sección se presenta otro remedio para el sobreajuste: la validación.

Se ha expuesto que la regularización consiste en estimar la penalización
que sufre cada modelo al aplicar un algoritmo de aprendizaje. En cambio,
la validación intenta estimar directamente el $E_{out}$.

Existe el concepto de conjunto de validación, el cual el similar al de
conjunto de test, pero con una sutil diferencia. Es desconocido para el
algoritmo de aprendizaje al igual que el de test, pero este conjunto sí toma
decisiones en el proceso de aprendizaje.

Reglas empíricas dictan que un tamaño adecuado para este conjunto es el 20\%
de las muestras totales.

Se introduce el concepto de validación cruzada. Intuitivamente, se toman $M$
particiones distintas del conjunto de datos, con las proporciones deseadas
(suele ser 80-20 como se ha propuesto antes o también 50-50) y, para cada modelo
$M$, se entrena con un conjunto y se valida con el otro, y se entrena con el
otro y se valida con el uno. Se toma el valor mínimo de los errores producidos,
y este modelo será el que mejor estime $E_{out}$.

\subsection{Modelos de árboles: Boosting}
\label{subsec:boosting}

En esta sección se exponen las técnicas más modernas en el ámbito del
aprendizje estadístico, las técnicas ensemble. Estas técnicas se basan en
combinar modelos ``débiles'' (que sean poco mejores que una moneda) para unirlos
y formar uno de gran precisión.

Entre los métodos ensemble el clásico es AdaBoost, pero el estado del arte lo
forma ahora Gradient Boosting, más específicamente la librería \emph{XGBoost},
de~\citep{DBLP:journals/corr/ChenG16}. Está al tanto con el estado del arte
en este tipo de modelos, además de incluir modelos regularizados para prevenir
el sobreajuste. En la cita anterior se explica detalladamente cómo funciona
este tipo de métodos, además de los algoritmos propios que implementan.
