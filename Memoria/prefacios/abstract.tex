\begin{otherlanguage}{british}

In this project, the uncertainty that Real Time Strategy games like StarCraft
present is studied, applying statistical and learning techniques.
With all of this, uncertainty can be evaluated for next optimizations,
and thus agents able to evaluate this uncertainty and modify its strategy could
be developed.

\subsubsection*{Statistical Learning}

Firstly, the concept of \emph{statistical learning} is defined. It is a branch
of Mathematics that aims to learn unknown functions from data using statistical
techniques. This type of learning can be modelled mathematically to solve a
large set of problems in Science, Engineering, Economy, etc.

In-sample error and out-sample error concepts are introduced and used
throughout the entire project. In-sample error measure the accuracy based on
points that are inside the input space. Out-sample error measure the accuracy
on unknown points.

Then, the feasibility of learning in finite hypothesis sets is proven, mainly
using Hoeffding’s results: deviation between in-sample and out-sample errors
can be bounded, but exists a big limitation in these results. A hypothesis
function is set beforehand, so the desviation will depends on the data set.

After that, generalizing to infinite hypothesis sets is needed. These are the
most common sets in real life. For this purpose, the works of Vapnik and
Chervonenkis are presented. Their work from the 60’s to the 90’s involved
investigating in order to prove generalization and performing further relevant
research for the statistical learning theory. Their most important work is the
Vapnik-Chervonenkis bound. It bounds the out-sample error with the in-sample
one and a penalty term. This bound relies on the Vapnik-Chervonenkis dimension
of the model: an integer that represents the maximum number of points that a
model can shatter.

Using the Vapnik-Chervonenkis bound it is possible to measure the sample complexity and the
model complexity. In addition, the test set is introduced and analyzed using
Hoeffding's inequality.

Furthermore, the bias-var analysis is introduced in this chapter.
It models the out-of-sample error using the bias and the variance of the model,
it is considerably useful to develop new learning techniques and prevent overfitting.
It exists two approaches to achieve this: reduce bias without increase variance,
which is so application-related, and reduce variance without increase bias,
which is solved with general techniques.

Overfitting is the main problem of learning. It occurs when in-sample error is
great, but it does not generalizes at all. This is due to the fact that the
model is too complex and able to fit the noise of the data, giving a final
hypothesis that is worse than a simpler one. There are two types of noise:
stochastic and deterministic. Stochastic is random noise, and deterministic
is the region that the final hypothesis can't fit.

Validation tries to estimate de out-sample error using a similar idea of the
test set.

To combat overfitting Regularization could be used. Basically it constraints
the algorithms to improve out-of-sample error instead of minimizing in-sample
one. These methods are usually heuristics so there is a bit of art in this
field. A classical method of regularization is weight-decay. It constraints
the range of the coefficients of a final hypothesis.

In this chapter, the validation concept and the cross-validation method for
estimating out-of sample error are explained.


Lastly, in the mathematical background, the ensemble methods for learning are
described. These methods combine weak learners (only slightly better than
throwing a coin) to produce strong learners. Gradient Boosting is the reference
in this project.

\subsubsection*{Study of the uncertainty}

When anyone has to solve a data science problem the first step is understanding
the data that could be obtained and the problem that will be solved. There are
six StarCraft replays relational databases with a substantial amount of data,
and thus the first step is understanding them and selecting a subset.

After that, the data have to be preprocessed in order to be able to work with
statistical software. Dumping from a database, extracting the important features
and organizing them are basically an art. Whoever handles the data best could
have an important advantage.

The next step is doing an Exploratory Data Analysis, since obtaining a general
vision of the data and their distribution is essential. R is used for the
analysis because it is a good reference in statistics software. Barplots and
time plots are good tools to handle these data, and they could give some ideas to
fight them. As the dataset is too large, these ideas involve calculating the
slope of the regression line for each feature and each player, and calculating
the Area Under Curve of the feature graphs.

Then, XGBoost, the library used in this project, is described. It is an open
source library that implements the Gradient Boosting algorithm in a definitely
efficient way. It is the state of the art in learning libraries and it has great
importance in Kaggle competitions. It is used in some companies as well.

It provides a very efficient and effective way to classify data with ensemble
learning, with regularization techniques and parallel approaches; hence,
it is rather fast and provides excellent results.

Lastly, the results of playing with the data are explained, and conclusions
will be presented in the last chapter.

\subsubsection*{Conclusions}

The main conclusion of this work is that the time is not the main feature to
predict uncertainty: observed values from one player to the other one are
more useful if a powerful prediction model is used.




\end{otherlanguage}
